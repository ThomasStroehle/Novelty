{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThomasStroehle/Novelty/blob/main/novelty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntXU-9KFPRK1"
      },
      "source": [
        "# Installation and loading of the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "agHXUFfmPKuW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install sentence-transformers\n",
        "!pip install requests_html\n",
        "!pip install gensim\n",
        "!pip install sentence_transformers\n",
        "!pip install sacremoses\n",
        "!pip install simcse\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRCAbGiePbKU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6UZCPHPQPDW"
      },
      "source": [
        "# Global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5o1gOh1IQRRY"
      },
      "outputs": [],
      "source": [
        "# dataset of idea contest\n",
        "DATASET_FILE = \"/content/drive/MyDrive/novelty/data.csv\" \n",
        "# announcement text of idea contest\n",
        "PRIORS_FILE = \"/content/drive/MyDrive/novelty/announcement_ideas_contest.csv\" \n",
        "# novelty ratings from prolific experiment\n",
        "PROLIFIC_FILE = \"/content/drive/MyDrive/novelty/novelty_prolific.csv\"\n",
        "# backup data\n",
        "BACKUP_DATA_FILE = \"/content/drive/MyDrive/novelty/data.pickle\"\n",
        "# backup prior hand\n",
        "BACKUP_HAND_FILE = \"/content/drive/MyDrive/novelty/prior_hand.pickle\"\n",
        "# backup prior scraped\n",
        "BACKUP_SCRAPED_FILE = \"/content/drive/MyDrive/novelty/prior_scraped.pickle\"\n",
        "# result file\n",
        "RESULT_FILE = \"/content/drive/MyDrive/novelty/result.xlsx\"\n",
        "\n",
        "# question for additional prior web scrapping \n",
        "QUESTION = \"How could paper towels be improved?\" \n",
        "\n",
        "# Sentence Bert\n",
        "SBERT_MODEL = 'all-mpnet-base-v2'\n",
        "EMBEDDING_MAX_LENGTH = 512\n",
        "\n",
        "# GPT-3\n",
        "ENGINE_GPT3 = 'text-embedding-ada-002'\n",
        "OPENAI_API_KEY = ''\n",
        "\n",
        "# SIMCSE\n",
        "SIMCSE_MODEL = 'princeton-nlp/sup-simcse-roberta-large'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "d57fZd2SwHKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_xPUF7lEp-09",
        "outputId": "e1f8581e-5600-4361-c455-84488ff44cf6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(DATASET_FILE, index_col=0)\n",
        "prolific = pd.read_csv(PROLIFIC_FILE, index_col=0)\n",
        "prior_hand = pd.read_csv(PRIORS_FILE, on_bad_lines='warn')\n",
        "\n",
        "# Merging of Prolific Ratings into the data set\n",
        "data = pd.merge(data, prolific, on=\"id\", how=\"left\")\n",
        "\n",
        "# data['text'] = data['pain_point'] + data['solution']\n",
        "data['text'] = data['solution']\n",
        "data['text'] = data['text'].apply(lambda x: re.sub(r'https?://\\S+', '', x))\n",
        "\n",
        "data = data.set_index(['id'])\n",
        "data"
      ],
      "metadata": {
        "id": "k-y8lQaTsSOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of ideas: ' + str(data['text'].notnull().sum()))\n",
        "print('Number of ratings from researchers: ' + str(data['novelty_researcher'].notnull().sum()))\n",
        "print('Number of ratings from experts: ' + str(data['novelty_expert'].notnull().sum()))\n",
        "print('Number of ratings from research participants (prolific): ' + str(data['novelty_prolific'].notnull().sum()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1wObkR6k4Tsh",
        "outputId": "b250238b-9da2-4611-d2cb-53a652cd19ae"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of ideas: 232\n",
            "Number of ratings from researchers: 225\n",
            "Number of ratings from experts: 58\n",
            "Number of ratings from research participants (prolific): 203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducing data\n",
        "data = data[data['novelty_researcher'].notnull() | data['novelty_expert'].notnull() | data['novelty_prolific'].notnull()]\n",
        "print('Number of reduced ideas: ' + str(data['text'].notnull().sum()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RtTpXSus7C9d",
        "outputId": "1628684d-4b5a-40f7-a6ba-8b8f59752a87"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of reduced ideas: 229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH-o2wMyPrLT"
      },
      "source": [
        "# Web scraping for prior knowldege"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "V9zwA7qUPqBU"
      },
      "outputs": [],
      "source": [
        "from requests_html import HTMLSession\n",
        "from requests import Response\n",
        "import requests\n",
        "import urllib\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "\n",
        "def scrape_html(url: str, debug: bool) -> Response:\n",
        "    session = HTMLSession()\n",
        "    response = session.get(url, timeout=10)\n",
        "    if debug:\n",
        "        print(f\"Scraped {url}\")\n",
        "    return response, url\n",
        "\n",
        "def google_query_url(query: str, competition_date: str, page: int) -> str:\n",
        "    if competition_date:\n",
        "        query = f\"{query} before:{competition_date}\"\n",
        "    else:\n",
        "        query = f\"{query}\"\n",
        "    sanitised_query = urllib.parse.quote_plus(query)\n",
        "    return f\"https://www.google.com/search?q={sanitised_query}&start={page*10}\"\n",
        "\n",
        "def find_query_urls(query: str, competition_date: str, debug: bool, pages: int) -> list:\n",
        "    links = []\n",
        "    for page in range(pages):\n",
        "      query_url = google_query_url(query, competition_date, page)\n",
        "      google_search_html, _ = scrape_html(query_url, debug)\n",
        "      links.extend(list(google_search_html.html.absolute_links))\n",
        "    google_urls = ('https://www.google.',\n",
        "                   'https://google.',\n",
        "                   'https://webcache.googleusercontent.',\n",
        "                   'http://webcache.googleusercontent.',\n",
        "                   'https://policies.google.',\n",
        "                   'https://support.google.',\n",
        "                   'https://maps.google.',\n",
        "                   'https://translate.google.')\n",
        "    for url in links[:]:\n",
        "        if url.startswith(google_urls):\n",
        "            links.remove(url)\n",
        "    return links\n",
        "\n",
        "\n",
        "def scrape_html_paragraph_text(scraped_html) -> list:\n",
        "    paragraphs = scraped_html.html.find('p', first=False)\n",
        "    paragraph_texts = [p.text for p in paragraphs]\n",
        "    paragraph_texts_filtered = list(filter(lambda p: bool(re.match(r\"^[A-z].{10,800}([.?!])$\", p)), paragraph_texts))\n",
        "    return paragraph_texts_filtered\n",
        "\n",
        "\n",
        "def scrape_paragraphs_from_query(query: str, competition_date: str, debug: bool, progress_bar: bool, pages: int) -> list:\n",
        "    links = find_query_urls(query, competition_date, debug, pages)\n",
        "    ex = ThreadPoolExecutor(max_workers=16)\n",
        "    futures = [ex.submit(scrape_html, url, debug) for url in links]\n",
        "    bar = tqdm(total=len(links), smoothing=1, disable = not progress_bar)\n",
        "    all_paragraphs = []\n",
        "    for future in as_completed(futures):\n",
        "      try:\n",
        "        scraped, url = future.result()\n",
        "        paragraphs = scrape_html_paragraph_text(scraped)\n",
        "        all_paragraphs.extend([(p, url) for p in paragraphs])\n",
        "        bar.update()\n",
        "      except requests.exceptions.RequestException as e:\n",
        "        if debug:\n",
        "          print(f\"Request error while scraping: {e}\")\n",
        "        bar.update()\n",
        "        continue\n",
        "      except Exception as e:\n",
        "        if debug:\n",
        "          print(f\"Other error while scraping: {e}\")\n",
        "        bar.update()\n",
        "        continue\n",
        "    bar.close()\n",
        "    return all_paragraphs\n",
        "\n",
        "\n",
        "def web_scrape(query: str, competition_date: str = None, debug: bool = False, progress_bar: bool = True, pages=1) -> list:\n",
        "    if debug:\n",
        "        print(f\"Scraping sites for {query}...\")\n",
        "    paragraphs_url = scrape_paragraphs_from_query(query, competition_date, debug, progress_bar, pages)\n",
        "    if debug:\n",
        "        print(f\"Done scraping, found {len(paragraphs_url)} paragraphs.\")\n",
        "    paragraphs = [p for p, url in paragraphs_url]\n",
        "    urls = [url for p, url in paragraphs_url]\n",
        "    results = {'paragraph': paragraphs, 'url': urls}\n",
        "    return results, query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8ThpIu2-voz"
      },
      "outputs": [],
      "source": [
        "prior_paragraphs, _ = web_scrape(query=QUESTION, pages=5)\n",
        "prior_scraped = pd.DataFrame()\n",
        "prior_scraped['text'] = prior_paragraphs['paragraph']\n",
        "\n",
        "# Prior webscraped knowledge\n",
        "prior_scraped.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcNxm1y-Pn4i"
      },
      "source": [
        "# Embeddings calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-3 Embeddings"
      ],
      "metadata": {
        "id": "eHD6GjGNr7Lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gpt3_embedding(text):\n",
        "  model_output = openai.Embedding.create(\n",
        "    input=text.to_list(),\n",
        "    engine=ENGINE_GPT3,\n",
        "    convert_to_numpy=True)\n",
        "\n",
        "  embedding = []\n",
        "  for i in range(len(model_output['data'])):\n",
        "    embedding.append(model_output['data'][i]['embedding'])\n",
        "\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "Y2rFKFdpv47a"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "data['embedding_ada002']  = get_gpt3_embedding(data['text'])\n",
        "prior_scraped['embedding_ada002']  = get_gpt3_embedding(prior_scraped['text'])\n",
        "prior_hand['embedding_ada002']  = get_gpt3_embedding(prior_hand['text'])"
      ],
      "metadata": {
        "id": "LhgjHJ3dr6E3"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKifbaeL-npv"
      },
      "source": [
        "## sBERT Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plRoCvBr-nav"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "\n",
        "# Load sBERT embedding model\n",
        "embedding_model = SentenceTransformer(SBERT_MODEL, device=device)\n",
        "embedding_model.max_seq_length = EMBEDDING_MAX_LENGTH\n",
        "\n",
        "# Abbreviation of the encode function\n",
        "def get_sbert_embedding(data_text):\n",
        "  embedding = embedding_model.encode(\n",
        "    data_text.to_list(), \n",
        "    show_progress_bar=True, \n",
        "    convert_to_numpy=True).tolist()\n",
        "  return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vbWXT6AA0HR"
      },
      "outputs": [],
      "source": [
        "# Embedd all data using sBERT\n",
        "data['embedding_sbert'] = get_sbert_embedding(data['text'])\n",
        "prior_scraped['embedding_sbert'] = get_sbert_embedding(prior_scraped['text'])\n",
        "prior_hand['embedding_sbert'] = get_sbert_embedding(prior_hand['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Doc2Vec Embeddings"
      ],
      "metadata": {
        "id": "mXuOKe-8GqRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "\n",
        "# Create training corpus by using all paragraphs from competition and scraped priors\n",
        "corpus = data['text'].to_list() + \\\n",
        "          prior_scraped['text'].to_list() + \\\n",
        "          prior_hand['text'].to_list()\n",
        "corpus = list(map(lambda x: word_tokenize(x), corpus))\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus)]\n",
        "\n",
        "# Train gensim Doc2Vec model on corpus\n",
        "doc2vec_model = Doc2Vec(documents, vector_size=256, window=5, min_count=2, workers=4, epochs=50)\n",
        "\n",
        "def get_doc2vec_embedding(data_text):\n",
        "  embedding = [doc2vec_model.infer_vector(word_tokenize(paragraph), epochs=40) for \\\n",
        "  paragraph in data_text.tolist()]\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "TwRWf1DAFmSm"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate paragraph embeddings from Doc2Vec model\n",
        "data['embedding_doc2vec'] = get_doc2vec_embedding(data['text'])\n",
        "prior_scraped['embedding_doc2vec'] = get_doc2vec_embedding(prior_scraped['text'])\n",
        "prior_hand['embedding_doc2vec'] = get_doc2vec_embedding(prior_hand['text'])"
      ],
      "metadata": {
        "id": "eM5Lh9J_Vcde"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SIMCSE Embeddings"
      ],
      "metadata": {
        "id": "MK_IsFfDEkpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from simcse import SimCSE\n",
        "simcse_model = SimCSE(SIMCSE_MODEL)\n",
        "\n",
        "def get_simcse_embedding(data_text):\n",
        "  embedding = [simcse_model.encode(paragraph, max_length=EMBEDDING_MAX_LENGTH, return_numpy=True) \n",
        "    for paragraph in data_text.tolist()]\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "opcTF9NMMouu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['embedding_simcse'] = get_simcse_embedding(data['text'])\n",
        "prior_scraped['embedding_simcse'] = get_simcse_embedding(prior_scraped['text'])\n",
        "prior_hand['embedding_simcse'] = get_simcse_embedding(prior_hand['text'])"
      ],
      "metadata": {
        "id": "NaLevfj3G0vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Embeddings"
      ],
      "metadata": {
        "id": "6QzKYPw7BJa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_pickle(BACKUP_DATA_FILE)\n",
        "prior_scraped.to_pickle(BACKUP_SCRAPED_FILE)\n",
        "prior_hand.to_pickle(BACKUP_HAND_FILE)"
      ],
      "metadata": {
        "id": "x46zZVLtBLrD"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_pickle(BACKUP_DATA_FILE)\n",
        "prior_scraped = pd.read_pickle(BACKUP_SCRAPED_FILE)\n",
        "prior_hand = pd.read_pickle(BACKUP_HAND_FILE)"
      ],
      "metadata": {
        "id": "-MuagvRqCAn2"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computation of Novelty and Outlier Scores"
      ],
      "metadata": {
        "id": "Hrtper222FQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Outlier Factor"
      ],
      "metadata": {
        "id": "R8nnhkfz2Mba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "def lof(embedding, prior, n_neighbors):\n",
        "  lof_model = LocalOutlierFactor(novelty=True, metric='cosine', n_neighbors=n_neighbors).fit(\n",
        "    prior.to_list())\n",
        "  return (-1)*lof_model.score_samples(embedding.to_list())"
      ],
      "metadata": {
        "id": "vFTSGyn73P31"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Bert Embeddings"
      ],
      "metadata": {
        "id": "q6jLOpqK9jQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraped prior pnowledge (Novelty Detection)\n",
        "data['lof_scraped_sbert'] = lof(data['embedding_sbert'], prior_scraped['embedding_sbert'], 10)\n",
        "# Announcement text of idea contest as prior knowledge (Novelty Detection)\n",
        "data['lof_hand_sbert'] = lof(data['embedding_sbert'], prior_hand['embedding_sbert'], 5)\n",
        "# Outlier Detection\n",
        "data['lof_outlier_sbert'] = lof(data['embedding_sbert'], data['embedding_sbert'], 5)"
      ],
      "metadata": {
        "id": "QhmcCLXbHXBR"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-3 Embeddings"
      ],
      "metadata": {
        "id": "HAx6654T3CHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraped prior pnowledge (Novelty Detection)\n",
        "data['lof_scraped_ada002'] = lof(data['embedding_ada002'], prior_scraped['embedding_ada002'], 10)\n",
        "# Announcement text of idea contest as prior knowledge (Novelty Detection)\n",
        "data['lof_hand_ada002'] = lof(data['embedding_ada002'], prior_hand['embedding_ada002'], 5)\n",
        "# Outlier Detection\n",
        "data['lof_outlier_ada002'] = lof(data['embedding_ada002'], data['embedding_ada002'], 5)"
      ],
      "metadata": {
        "id": "ZUH1uvYSy-vN"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doc2Vec"
      ],
      "metadata": {
        "id": "SyOnG0bS3GA2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "eibAJZFW1AS5"
      },
      "outputs": [],
      "source": [
        "# Scraped prior pnowledge (Novelty Detection)\n",
        "data['lof_scraped_doc2vec'] = lof(data['embedding_doc2vec'], prior_scraped['embedding_doc2vec'], 10)\n",
        "# Announcement text of idea contest as prior knowledge (Novelty Detection)\n",
        "data['lof_hand_doc2vec'] = lof(data['embedding_doc2vec'], prior_hand['embedding_doc2vec'], 5)\n",
        "# Outlier Detection\n",
        "data['lof_outlier_doc2vec'] = lof(data['embedding_doc2vec'], data['embedding_doc2vec'], 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SIMCSE"
      ],
      "metadata": {
        "id": "wVqMInWe-Z7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraped prior pnowledge (Novelty Detection)\n",
        "data['lof_scraped_simcse'] = lof(data['embedding_simcse'], prior_scraped['embedding_simcse'], 10)\n",
        "# Announcement text of idea contest as prior knowledge (Novelty Detection)\n",
        "data['lof_hand_simcse'] = lof(data['embedding_simcse'], prior_hand['embedding_simcse'], 5)\n",
        "# Outlier Detection\n",
        "data['lof_outlier_simcse'] = lof(data['embedding_simcse'], data['embedding_simcse'], 5)"
      ],
      "metadata": {
        "id": "ZaBTCg5CPFGe"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## kNN Distance Novelty and Outlier Detection"
      ],
      "metadata": {
        "id": "BjqjBWg_--85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kNN_novelty(data: pd.DataFrame, prior: pd.DataFrame, embedding: str, name: str, k = 5):\n",
        "  dist_mat = util.cos_sim(data[embedding].tolist(), prior[embedding].tolist()).numpy()\n",
        "  kNNs = np.flip(np.sort(dist_mat), axis=1)[:,0:k]\n",
        "  data.loc[:, [f'k{i}_{name}' for i in range(1,k+1)]] = 1-kNNs\n",
        "  k1_idx = np.argmax(dist_mat, axis=1)\n",
        "  k1_contents = [prior.iloc[idx, :]['text'] for idx in k1_idx]\n",
        "  data.loc[:, f'k1_{name}_text'] = k1_contents"
      ],
      "metadata": {
        "id": "OPIhOlb-rZlq"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kNN_outlier(data: pd.DataFrame, embedding: str, name: str, k = 5):\n",
        "  dist_mat = util.cos_sim(data[embedding].tolist(), data[embedding].tolist()).numpy()\n",
        "  kNNs = np.flip(np.sort(dist_mat), axis=1)[:,0:k]\n",
        "  data.loc[:, [f'k{i}_{name}' for i in range(1,k+1)]] = 1-kNNs\n",
        "  k1_idx = np.argmax(dist_mat, axis=1)\n",
        "  k1_contents = [data.iloc[idx, :]['text'] for idx in k1_idx]\n",
        "  data.loc[:, f'k1_{name}_text'] = k1_contents"
      ],
      "metadata": {
        "id": "Y2EjofxQ7LtW"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kNN_novelty(data, prior_scraped, embedding='embedding_sbert', name='scraped_sbert')\n",
        "kNN_novelty(data, prior_hand, embedding='embedding_sbert', name='hand_sbert')\n",
        "kNN_outlier(data, embedding='embedding_sbert', name='outlier_sbert')"
      ],
      "metadata": {
        "id": "XlML8Xvz_36y"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kNN_novelty(data, prior_scraped, embedding='embedding_doc2vec', name='scraped_doc2vec')\n",
        "kNN_novelty(data, prior_hand, embedding='embedding_doc2vec', name='hand_doc2vec')\n",
        "kNN_outlier(data, embedding='embedding_doc2vec', name='outlier_doc2vec')"
      ],
      "metadata": {
        "id": "oF4TMQLu_5Qq"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kNN_novelty(data, prior_scraped, embedding='embedding_simcse', name='scraped_simcse')\n",
        "kNN_novelty(data, prior_hand, embedding='embedding_simcse', name='hand_simcse')\n",
        "kNN_outlier(data, embedding='embedding_simcse', name='outlier_simcse')"
      ],
      "metadata": {
        "id": "o8BfFSgE_68y"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kNN_novelty(data, prior_scraped, embedding='embedding_ada002', name='scraped_ada002')\n",
        "kNN_novelty(data, prior_hand, embedding='embedding_ada002', name='hand_ada002')\n",
        "kNN_outlier(data, embedding='embedding_ada002', name='outlier_ada002')"
      ],
      "metadata": {
        "id": "V-GQ4zrvsBuq"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Novelty and Outlier Scores"
      ],
      "metadata": {
        "id": "MEFR4nmPXcna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_pickle(BACKUP_DATA_FILE)\n",
        "prior_scraped.to_pickle(BACKUP_SCRAPED_FILE)\n",
        "prior_hand.to_pickle(BACKUP_HAND_FILE)\n"
      ],
      "metadata": {
        "id": "hOV0y3MfzmAB"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_pickle(BACKUP_DATA_FILE)\n",
        "prior_scraped = pd.read_pickle(BACKUP_SCRAPED_FILE)\n",
        "prior_hand = pd.read_pickle(BACKUP_HAND_FILE)"
      ],
      "metadata": {
        "id": "swwOT7yTIs1U"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation Analysis"
      ],
      "metadata": {
        "id": "TOzpXvxtAGXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation function, which creates a correlation table (correlation value and pvalue) on specific columns."
      ],
      "metadata": {
        "id": "FBNu-nJqgAi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correlation_analysis(data, columns_string):\n",
        "  correlation = []\n",
        "  p_value = []\n",
        "  column_i = []\n",
        "  column_j = []\n",
        "  for i in columns_string:\n",
        "    for j in columns_string:\n",
        "      r = stats.spearmanr(data[i], data[j], nan_policy = 'omit')\n",
        "      correlation.append(np.round(r.correlation,2))\n",
        "      p_value.append(np.round(r.pvalue,2))\n",
        "      column_i.append(i)\n",
        "      column_j.append(j)\n",
        "\n",
        "  result = pd.DataFrame({\"i\":column_i,\n",
        "                         \"j\":column_j,\n",
        "                         \"correlation\": correlation,\n",
        "                         \"p_value\": p_value})\n",
        "  return result"
      ],
      "metadata": {
        "id": "TSVFmCJoFTsu"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Division of the data set into ideas with a short number of tokens and a long number of tokens."
      ],
      "metadata": {
        "id": "IiJ2-14wgTlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['number_tokens'] = data['text'].apply(nltk.word_tokenize).apply(len)\n",
        "data['short'] = data['number_tokens'].apply(lambda x: True if x <= data['number_tokens'].median() else False)\n",
        "\n",
        "data_short = data[data['short'] == True]\n",
        "data_long = data[data['short'] == False]\n",
        "\n",
        "data.short.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sO0Er5oAOCSO",
        "outputId": "cfeb194c-5c6e-4974-b3b1-85b631dd8db1"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True     115\n",
              "False    114\n",
              "Name: short, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of ideas "
      ],
      "metadata": {
        "id": "sZVCP3PcgrRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of ideas: ' + str(data['text'].notnull().sum()))\n",
        "print('Number of ratings from researchers: ' + str(data['novelty_researcher'].notnull().sum()))\n",
        "print('Number of ratings from experts: ' + str(data['novelty_expert'].notnull().sum()))\n",
        "print('Number of ratings from research participants (prolific): ' + str(data['novelty_prolific'].notnull().sum()))\n",
        "#\n",
        "print('Number of short ideas: ' + str(data_short['text'].notnull().sum()))\n",
        "print('Number of short ratings from researchers: ' + str(data_short['novelty_researcher'].notnull().sum()))\n",
        "print('Number of short ratings from experts: ' + str(data_short['novelty_expert'].notnull().sum()))\n",
        "print('Number of short ratings from research participants (prolific): ' + str(data_short['novelty_prolific'].notnull().sum()))\n",
        "#\n",
        "print('Number of long ideas: ' + str(data_long['text'].notnull().sum()))\n",
        "print('Number of long ratings from researchers: ' + str(data_long['novelty_researcher'].notnull().sum()))\n",
        "print('Number of long ratings from experts: ' + str(data_long['novelty_expert'].notnull().sum()))\n",
        "print('Number of long ratings from research participants (prolific): ' + str(data_long['novelty_prolific'].notnull().sum()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0Rtec-5cOq9y",
        "outputId": "49f98182-12a0-429e-ae8d-4ac4672429b9"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of ideas: 229\n",
            "Number of ratings from researchers: 225\n",
            "Number of ratings from experts: 58\n",
            "Number of ratings from research participants (prolific): 203\n",
            "Number of short ideas: 115\n",
            "Number of short ratings from researchers: 112\n",
            "Number of short ratings from experts: 23\n",
            "Number of short ratings from research participants (prolific): 101\n",
            "Number of long ideas: 114\n",
            "Number of long ratings from researchers: 113\n",
            "Number of long ratings from experts: 35\n",
            "Number of long ratings from research participants (prolific): 102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing the correlation tables"
      ],
      "metadata": {
        "id": "rq1aZju4g48U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_string = ['novelty_prolific', 'novelty_researcher', 'novelty_expert']\n",
        "\n",
        "for i in ['k1', 'k3', 'lof']:\n",
        "  for j in ['scraped', 'hand', 'outlier']:\n",
        "    for k in ['sbert', 'doc2vec', 'simcse', 'ada002']:\n",
        "      columns_string.append(i + '_' + j + '_' + k)\n",
        "\n",
        "result_all = correlation_analysis(data, columns_string)\n",
        "correlation_all = result_all.pivot(index=\"i\",columns=\"j\", values=\"correlation\")\n",
        "correlation_all = correlation_all.reset_index(level=0)\n",
        "pvalue_all = result_all.pivot(index=\"i\",columns=\"j\", values=\"p_value\")\n",
        "pvalue_all = pvalue_all.reset_index(level=0)\n",
        "\n",
        "result_short = correlation_analysis(data_short, columns_string)\n",
        "correlation_short = result_short.pivot(index=\"i\",columns=\"j\", values=\"correlation\")\n",
        "correlation_short = correlation_short.reset_index(level=0)\n",
        "pvalue_short = result_short.pivot(index=\"i\",columns=\"j\", values=\"p_value\")\n",
        "pvalue_short = pvalue_short.reset_index(level=0)\n",
        "\n",
        "result_long = correlation_analysis(data_long, columns_string)\n",
        "correlation_long = result_long.pivot(index=\"i\",columns=\"j\", values=\"correlation\")\n",
        "correlation_long = correlation_long.reset_index(level=0)\n",
        "pvalue_long = result_long.pivot(index=\"i\",columns=\"j\", values=\"p_value\")\n",
        "pvalue_long = pvalue_long.reset_index(level=0)"
      ],
      "metadata": {
        "id": "Jj9aIBRjQYx9"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing results to excel"
      ],
      "metadata": {
        "id": "cXmv4-2WTYB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with pd.ExcelWriter(RESULT_FILE) as writer:\n",
        "    result_all.to_excel(writer, sheet_name=\"result_all\", index=False)\n",
        "    correlation_all.to_excel(writer, sheet_name=\"correlation_all\", index=False)\n",
        "    pvalue_all.to_excel(writer, sheet_name=\"pvalue_all\", index=False)\n",
        "    result_short.to_excel(writer, sheet_name=\"result_short\", index=False)\n",
        "    correlation_short.to_excel(writer, sheet_name=\"correlation_short\", index=False)\n",
        "    pvalue_short.to_excel(writer, sheet_name=\"pvalue_short\", index=False)\n",
        "    result_long.to_excel(writer, sheet_name=\"result_long\", index=False)\n",
        "    correlation_long.to_excel(writer, sheet_name=\"correlation_long\", index=False)\n",
        "    pvalue_long.to_excel(writer, sheet_name=\"pvalue_long\", index=False)"
      ],
      "metadata": {
        "id": "9J8-w4GpKmGh"
      },
      "execution_count": 173,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ikdsv5ZFc8Yv",
        "BR17OFKSNdxC"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}